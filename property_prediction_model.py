# -*- coding: utf-8 -*-
"""Property prediction model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EXmUYLCNlk2aEIMeC-BvQRv9XZPME84h
"""

!pip install PyPDF2

"""**This algorithm helps to extract the data from PDFs and save it as an excel file**"""

import os
import PyPDF2
import pandas as pd
import re


def extract_pdf_text(pdf_file):
    pdf_file = open(pdf_file, 'rb')
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    for page in range(len(pdf_reader.pages)):
        text += pdf_reader.pages[page].extract_text()
    pdf_file.close()
    return text

def extract_property_values(text, property_name):
    pattern = f'{property_name}.*?([\d.]+)'
    match = re.search(pattern, text, re.DOTALL)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            return None
    else:
        return None


def extract_property_values_string(text, property_name):
    pattern = f'{property_name}.*?([A-Za-z, ]+)'
    match = re.search(pattern, text, re.DOTALL)
    if match:
      return match.group(1)
    else:
      return None


# Store the path of the folder containing the PDF files
pdf_folder = "/content/PDFs"
pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]
texts = [extract_pdf_text(os.path.join(pdf_folder, f)) for f in pdf_files]

# Extract the values of tensile modulus, elongation at break, processing methods, delivery form, special characteristics, and applications

#Input parameters for processing or Injection molding
pre_drying_temperature =                                        [extract_property_values(text, "Pre-drying - Temperature") for text in texts]
pre_drying_time =                                                     [extract_property_values(text, "Pre-drying - Time") for text in texts]
mold_temperature =                                                  [extract_property_values(text, "Mold temperature") for text in texts]
feed_temperature =                                                  [extract_property_values(text, "Feed temperature") for text in texts]
zone_1 =                                                                    [extract_property_values(text, "Zone 1") for text in texts]
zone_2 =                                                                   [extract_property_values(text, "Zone 2") for text in texts]
nozzle_temperature =                                               [extract_property_values(text, "Nozzle temperature") for text in texts]
melt_flow_index_MFI =                                           [extract_property_values(text, 'Melt flow index, MFI') for text in texts]
melt_volume_flow_rate_MVR =                                [extract_property_values(text, 'Melt colume-flow rate, MVR') for text in texts]
temperature =                                                           [extract_property_values(text, 'Temperature') for text in texts]
load =                                                                        [extract_property_values(text, 'Load') for text in texts]
molding_shrinkage_parallel =                                    [extract_property_values(text, 'Molding shrinkage, parallel') for text in texts]
molding_shrinkage_normal =                                     [extract_property_values(text, 'Molding shrinkage, normal') for text in texts]
melt_temperature =                                                  [extract_property_values(text, 'Melt temperature') for text in texts]

#Mechanical properties
tensile_strength =                                                    [extract_property_values(text, 'Tensile Strength') for text in texts]
elongation_at_break =                                              [extract_property_values(text, 'Elongation at Break') for text in texts]
tensile_modulus =                                                     [extract_property_values(text, 'Tensile Modulus') for text in texts]
stress_at_break =                                                    [extract_property_values(text, 'Stress at break') for text in texts]
strain_at_break =                                                     [extract_property_values(text, 'Strain at break') for text in texts]
yield_stress =                                                           [extract_property_values(text, 'Yield stress') for text in texts]
yield_strain =                                                            [extract_property_values(text, 'Yield strain') for text in texts]
charpy_impact_strength_23c =                                [extract_property_values(text, 'Charpy notched impact strength, +23°C') for text in texts]
izod_impact_strength_23c =                                    [extract_property_values(text, 'Izod notched impact strength, +23°C') for text in texts]
flexural_modulus_23c =                                            [extract_property_values(text, 'Flexural modulus, 23°C') for text in texts]
shore_A_hardness =                                                 [extract_property_values(text, 'Shore A Hardness') for text in texts]
shore_D_hardness =                                                 [extract_property_values(text, 'Shore D Hardness') for text in texts]
density =                                                                   [extract_property_values(text, 'Density') for text in texts]

#Electrical properties
volume_resistivity =                                                  [extract_property_values(text, 'Volume resistivity') for text in texts]
surface_resistivity =                                                [extract_property_values(text, 'Surface resistivity') for text in texts]
comparative_trackinig_index =                                 [extract_property_values(text, 'Comparative tracking index') for text in texts]
electric_strength =                                                  [extract_property_values(text, 'Electric strength') for text in texts]

#Thermal properties
melting_temperature_10c_min =                              [extract_property_values(text, 'Melting temperature, 10°C/min') for text in texts]
temp_deflection_under_load_1_8_MPa =                 [extract_property_values(text, 'Temp. of deflection under load, 1.80 MPa') for text in texts]
temp_deflection_under_load_0_45_MPa =              [extract_property_values(text, 'Temp. of deflection under load, 0.45 MPa') for text in texts]
coeff_linear_therm_expansion_parallel =                [extract_property_values(text, 'Coeff. of linear therm. expansion, parallel') for text in texts]
coeff_linear_therm_expansion_normal =                 [extract_property_values(text, 'Coeff. of linear therm. expansion, normal') for text in texts]


processing_methods =                                              [extract_property_values_string(text, "Processing") for text in texts]
delivery_form =                                                        [extract_property_values_string(text, "Delivery form") for text in texts]
special_characteristics =                                         [extract_property_values_string(text, "Special characteristics") for text in texts]
applications =                                                           [extract_property_values_string(text, "Applications") for text in texts]

# Convert the list of extracted data into a Pandas DataFrame
# df = pd.DataFrame({'file_name': pdf_files, 'tensile_modulus': tensile_modulus, 'elongation_at_break': elongation_at_break, 'tensile_strength': tensile_strength, 'processing_methods': processing_methods, 'delivery_form': delivery_form, 
#                    'special_characteristics': special_characteristics, 'applications': applications})

df = pd.DataFrame({'file_name': pdf_files,'Pre_drying_temperature (°C)': pre_drying_temperature,'Pre_drying_time (h)':pre_drying_time, 'mold_temperature (°C)': mold_temperature, 
                   'feed_temperature  (°C)': feed_temperature, 'zone_1 (°C)': zone_1,'zone_2 (\u2103)': zone_2, 'nozzle_temperature (°C)': nozzle_temperature, 
                   'melt_flow_index_MFI (g/10min)': melt_flow_index_MFI, 'melt_volume_flow_rate_MVR (cm3/10min)': melt_volume_flow_rate_MVR, 'temperature (°C)' : temperature, 'load (kg)': load,
                   'molding_shrinkage_parallel (%)': molding_shrinkage_parallel, 'molding_shrinkage_normal (%)': molding_shrinkage_normal, 'melt_temperature (°C)': melt_temperature, 
                   'tensile_strength (MPa)':tensile_strength,'elongation_at_break (%)': elongation_at_break, 'tensile_modulus (MPa)': tensile_modulus,  'stress_at_break (MPa)': stress_at_break, 'strain_at_break (%)': strain_at_break,
                   'yield_stress (MPa)': yield_stress, 'yield_strain (%)': yield_strain, 'charpy_impact_strength_23c (kJ/m2)':charpy_impact_strength_23c, 'izod_impact_strength_23c (kJ/m2)': izod_impact_strength_23c, 
                   'flexural_modulus_23c (MPa)': flexural_modulus_23c, 'shore_A_hardness': shore_A_hardness, 'shore_D_hardness': shore_D_hardness, 'density (kg/m3)': density, 'volume_resistivity (Ohm*m)': volume_resistivity,
                   'surface_resistivity(Ohm) ':surface_resistivity, 'comparative_trackinig_index': comparative_trackinig_index, 'electric_strength (kV/mm)': electric_strength, 'melting_temperature_10c_min (°C)': melting_temperature_10c_min,
                   'temp_deflection_under_load_1_8_MPa (°C)': temp_deflection_under_load_1_8_MPa, 'temp_deflection_under_load_0_45_MPa (°C)': temp_deflection_under_load_0_45_MPa,
                   'coeff_linear_therm_expansion_parallel (E-6/K)': coeff_linear_therm_expansion_parallel, 'coeff_linear_therm_expansion_normal (E-6/K)': coeff_linear_therm_expansion_normal,
                   'processing_methods': processing_methods, 'delivery_form': delivery_form, 'special_characteristics': special_characteristics, 'applications': applications})


# Write the DataFrame to an excel file
df.to_csv("extracted_data.csv", index=False)

"""**Support vector regression algorithm too predict "TENSILE MODULUS" of a material**"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_excel("/content/Book5.xlsx")
df = df.drop(['file_name'], axis = 1)
df_copy = df.copy()
df_copy.fillna(0, inplace=True)
# df = df.astype(float)
df_copy

X = df_copy.drop(['TYPE', 'tensile_strength (MPa)', 'elongation_at_break (%)', 'tensile_modulus (MPa)'], axis=1)
X.fillna(0, inplace = True)
y = df_copy['tensile_modulus (MPa)']
y.fillna(0, inplace = True)

# split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#Feature scaling
st_x = StandardScaler()
x_train = st_x.fit_transform(X_train)
x_test = st_x.transform(X_test)

# fit the SVM model to the training data
model = SVR(kernel='linear')
model.fit(x_train, y_train)

#Predict the response for test dataset
y_pred = model.predict(x_test)

# Calculate the mean squared error of the predictions
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# # Evaluate the model
score = r2_score( y_test, y_pred)
print("R-squared score: ", score)

# evaluate the model on the test data
# accuracy = metrics.accuracy_score(y_test, y_pred)
# print("Accuracy:", accuracy)

# cm = confusion_matrix(y_test,y_pred)
# print("Confusion matrix:", cm)

# plt.scatter(x_test[:,13], y_test)
plt.scatter(y_test, y_pred)
plt.xlabel('actual tensile modulus')
plt.ylabel('predicted tensile modulus')
plt.title('actual vs predicted')
plt.show()

"""**Linear regression algorithm to predict the value of "TENSILE MODULUS"**"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Load the data into a pandas DataFrame
df = pd.read_excel("/content/Book5.xlsx")
df = df.drop(['file_name'], axis = 1)
df_copy = df.copy()
df_copy.fillna(0, inplace=True)
# df = df.astype(float)
df_copy

X = df_copy.drop(['TYPE', 'tensile_strength (MPa)', 'elongation_at_break (%)', 'tensile_modulus (MPa)'], axis=1)
X.fillna(0, inplace = True)
y = df_copy['tensile_modulus (MPa)']
y.fillna(0, inplace = True)


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#Feature scaling
st_x = StandardScaler()
x_train = st_x.fit_transform(X_train)
x_test = st_x.transform(X_test)

# Train the model
regressor = LinearRegression()
regressor.fit(x_train, y_train)

# Predict the tensile modulus using the trained model
y_pred = regressor.predict(x_test)

# Calculate the mean squared error of the predictions
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Evaluate the model
score = regressor.score(x_test, y_test)
print("R-squared score: ", score)

# Plot the data points along with the hypothesis line
plt.scatter(x_test[:,1], y_test, color='black')
plt.scatter(x_test[:,1], y_pred, color='blue', linewidth=3)

# Add labels and title to the plot
plt.xlabel("Input feature 1")
plt.ylabel("Target value")
plt.title("Multivariate linear regression")


# Show the plot
plt.show()

"""**Algorithm to measure the performance of various kernels of SVR**

Prediction of "TENSILE MODULUS"
"""

import numpy as np
import pandas as pd
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Load data into a pandas dataframe
df = pd.read_excel("/content/Book5.xlsx")
df = df.drop(['file_name'], axis = 1)
df_copy = df.copy()
df_copy.fillna(0, inplace=True)
# df = df.astype(float)
df_copy

X = df_copy.drop(['TYPE', 'tensile_strength (MPa)', 'elongation_at_break (%)', 'tensile_modulus (MPa)'], axis=1)
X.fillna(0, inplace = True)
y = df_copy['tensile_modulus (MPa)']
y.fillna(0, inplace = True)


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#Feature scaling
st_x = StandardScaler()
x_train = st_x.fit_transform(X_train)
x_test = st_x.transform(X_test)

# Train the SVM classifier using different kernels
kernels = ["linear", "poly", "rbf", "sigmoid"]
for kernel in kernels:
    clf = SVR(kernel=kernel)
    clf.fit(x_train, y_train)
    
    # Predict the target on the test set
    y_pred = clf.predict(x_test)
    
    # # Calculate the mean squared error of the predictions
    # mse = mean_squared_error(y_test, y_pred)
    # print("Mean Squared Error:", mse)

    # # # Evaluate the model
    # score = r2_score( y_test, y_pred)
    # print("R-squared score: ", score)

    # Print the accuracy scores
    print(f"R2 score for kernel '{kernel}': {r2_score(y_test, y_pred)}\n") 
    
    print(f"mean squared error score for kernel '{kernel}': {mean_squared_error(y_test, y_pred)}\n")

"""**Random forest regression to predict the value of "TENSILE MODULUS"**"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score

# Load the data into a pandas DataFrame
df = pd.read_excel("/content/Book5.xlsx")
df = df.drop(['file_name'], axis = 1)
df_copy = df.copy()
df_copy.fillna(0, inplace=True)
# df = df.astype(float)
df_copy

X = df_copy.drop(['TYPE', 'tensile_strength (MPa)', 'elongation_at_break (%)', 'tensile_modulus (MPa)'], axis=1)
X.fillna(0, inplace = True)
y = df_copy['tensile_modulus (MPa)']
y.fillna(0, inplace = True)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#Feature scaling
st_x = StandardScaler()
x_train = st_x.fit_transform(X_train)
x_test = st_x.transform(X_test)

#Fitting descision tree classifier to the training set
classifier = RandomForestRegressor(n_estimators=1000, random_state=0)
classifier.fit(x_train, y_train)

# Predicting the test set result
y.pred = classifier.predict(x_test)

#printing the metrics
print("Mean Squared Error:", mean_squared_error(y_test, y.pred))
print("R^2 Score:", r2_score(y_test, y.pred))

# Create the plot
plt.scatter(x_test[:,13], y_test, color='black')
plt.scatter(x_test[:,13], y.pred, color='blue', linewidth=3)

plt.xlabel('feature')
plt.ylabel('predicted tensile modulus')
plt.title('feature vs predicted')
plt.show()

# accuracy = metrics.accuracy_score(y_test, y.pred)
# print("Accuracy:", accuracy)

# cm = confusion_matrix(y_test,y.pred)
# print("Confusion matrix:", cm)

new_df = pd.DataFrame({'y_test': y_test, 'y_pred': y.pred})
new_df

!pip install tensorflow
!pip install keras

, mean_absolute_error
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error


# Load the data
df = pd.read_excel("/content/Book5.xlsx")
df = df.drop(['file_name'], axis = 1)
df_copy = df.copy()
df_copy.fillna(0, inplace=True)
# df = df.astype(float)
df_copy

X = df_copy.iloc[:, 1 :-4].values
y1 = df_copy.iloc[:, -2].values
y2 = df_copy.iloc[:, -4].values


# Scale the input features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y1_train, y1_test, y2_train, y2_test = train_test_split(X, y1, y2, test_size=0.2)

# Define the model
input_dim = X.shape[1]
model = keras.Sequential([
    layers.Dense(32, input_dim=input_dim, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)
])

# Compile the model
model.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mse','mae', 'mape'])

# Train the model
history = model.fit(X_train, y1_train, epochs=3000, verbose=0)

# Evaluate the model on the test data
test_loss = model.evaluate(X_test, y1_test, verbose=0)
print('Test loss for tensile modulus prediction:', test_loss)

# Train the model for tensile strength prediction
history = model.fit(X_train, y2_train, epochs=3000, verbose=0)

# Evaluate the model on the test data
test_loss = model.evaluate(X_test, y2_test, verbose=0)
print('Test loss for tensile strength prediction:', test_loss)

# Calculate the mean squared error between the predicted and actual target values
mse = mean_squared_error(y_test, y_pred)
print('Mean Squared Error:', mse)

r2 = r2_score(y_test, y_pred)
print('R2 score:' , r2)

mea = mean_absolute_error(y_test, y_pred)
print("Mean absolute error:", mea)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense

# Load the data
df = pd.read_excel("/content/Book5.xlsx")
df = df.drop(['file_name'], axis = 1)
df_copy = df.copy()
df_copy.fillna(0, inplace=True)
# df = df.astype(float)
df_copy

X = df_copy.iloc[:, 1 :-4].values
y1 = df_copy.iloc[:, -2].values
y2 = df_copy.iloc[:, -4].values


# Split the data into training and test sets
X_train, X_test, y_modulus_train, y_modulus_test = train_test_split(X,y1, test_size=0.2)
_, _, y_strength_train, y_strength_test = train_test_split(X, y2, test_size=0.2)

# Scale the input features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the ANN model for tensile modulus prediction
model_modulus = Sequential()
model_modulus.add(Dense(128, activation='relu', input_dim=X.shape[1]))
model_modulus.add(Dense(64, activation='relu'))
model_modulus.add(Dense(32, activation='relu'))
model_modulus.add(Dense(1, activation='linear'))
model_modulus.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history_modulus = model_modulus.fit(X_train, y_modulus_train, epochs=1000, batch_size=32, validation_data=(X_test, y_modulus_test))

# Define the ANN model for tensile strength prediction
model_strength = Sequential()
model_strength.add(Dense(128, activation='relu', input_dim=X.shape[1]))
model_strength.add(Dense(64, activation='relu'))
model_strength.add(Dense(32, activation='relu'))
model_strength.add(Dense(1, activation='linear'))
model_strength.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history_strength = model_strength.fit(X_train, y_strength_train, epochs=1000, batch_size=32, validation_data=(X_test, y_strength_test))

# Use the trained models to make predictions on the test data
y_modulus_pred = model_modulus.predict(X_test)
y_strength_pred = model_strength.predict(X_test)

from sklearn.metrics import mean_squared_error

# Predict the target values using the ANN model
y_pred = model.predict(X_test)

# Calculate the mean squared error between the predicted and actual target values
mse = mean_squared_error(y_test, y_pred)
print('Mean Squared Error:', mse)

from sklearn.datasets import make_regression
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.ensemble import GradientBoostingRegressor
from numpy import std, mean
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV

# X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=0)
# Load the data
df = pd.read_excel("/content/Book5.xlsx")
df = df.drop(['file_name'], axis = 1)
df_copy = df.copy()
df_copy.fillna(0, inplace=True)
# df = df.astype(float)
df_copy

X = df_copy.drop(['TYPE', 'tensile_strength (MPa)', 'elongation_at_break (%)', 'tensile_modulus (MPa)'], axis=1)
X.fillna(0, inplace = True)
y = df_copy['tensile_modulus (MPa)']
y.fillna(0, inplace = True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)

cv = KFold(n_splits=10, shuffle = True, random_state=1)

GBR=GradientBoostingRegressor()
search_grid={'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1],'max_depth':[1,2,4],'subsample':[.5,.75,1],'random_state':[1]}
search=GridSearchCV(estimator=GBR,param_grid=search_grid,scoring='neg_mean_squared_error',n_jobs=1,cv=cv)

# n_scores  = cross_val_score(model, X, y, scoring = 'neg_mean_absolute_error', cv = cv,n_jobs= -1)

# print('MAE:  %3f  (%.3f)'  % (mean(n_scores), std(n_scores) ))


search.fit(X,y)
search.best_params_

GBR2=GradientBoostingRegressor(n_estimators=500,learning_rate=0.01,subsample=.1,max_depth=4,random_state=1)
score=np.mean(cross_val_score(GBR2,X,y,scoring='neg_mean_squared_error',cv=cv,n_jobs=1))
score

y.pred = search.predict(X_test)
y.pred

print('R score is :', r2_score(y_test, y.pred))
print('MEA :', mean_squared_error(y_test, y.pred))

df2 = pd.DataFrame({'y_test': y_test, 'y_pred': y.pred})
df2

from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score

# Lets split the data into 5 folds.  
# We will use this 'kf'(KFold splitting stratergy) object as input to cross_val_score() method
kf =KFold(n_splits=10, shuffle=True, random_state=42)

cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1

def rmse(score):
    rmse = np.sqrt(-score)
    print(f'rmse= {"{:.2f}".format(rmse)}')

score = cross_val_score(GradientBoostingRegressor(random_state= 42), X, y, cv=kf, scoring="neg_mean_squared_error")
print(f'Scores for each fold: {score}')
rmse(score.mean())

max_depth = [1,2,3,4,5,6,7,8,9,10]

for val in max_depth:
    score = cross_val_score(GradientBoostingRegressor(max_depth= val, random_state= 42), X, y, cv= kf, scoring="neg_mean_squared_error")
    print(f'For max depth: {val}')
    rmse(score.mean())

from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error
#
# Load the Boston Dataset
#

# Load the data
df = pd.read_excel("/content/Book5.xlsx")
df = df.drop(['file_name'], axis = 1)
df_copy = df.copy()
df_copy.fillna(0, inplace=True)
# df = df.astype(float)
df_copy

X = df_copy.drop(['TYPE', 'tensile_strength (MPa)', 'elongation_at_break (%)', 'tensile_modulus (MPa)'], axis=1)
X.fillna(0, inplace = True)
y = df_copy['tensile_strength (MPa)']
y.fillna(0, inplace = True)
#

crossvalidation=KFold(n_splits=10,shuffle=True,random_state=1)

# Create Training and Test Split
#
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)
#
# Standardize the dataset
#
sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)
#
# Hyperparameters for GradientBoostingRegressor
#
gbr_params = {'n_estimators': 2000,
          'max_depth': 5,
          'min_samples_split': 5,
          'learning_rate': 0.1,
          'loss': 'squared_error'}
#
# Create an instance of gradient boosting regressor
#
gbr = GradientBoostingRegressor(**gbr_params)
#
# Fit the model
#
gbr.fit(X_train_std, y_train)
#
# Print Coefficient of determination R^2
#
print("Model Accuracy: %.4f" % gbr.score(X_test_std, y_test))
#
# Create the mean squared error
#
mse = mean_squared_error(y_test, gbr.predict(X_test_std))
print("The mean squared error (MSE) on test set: {:.3f}".format(mse))

df1 = pd.DataFrame({'y_test': y_test, 'y_pred': gbr.predict(X_test_std)})
df1

import numpy as np
import matplotlib.pyplot as plt
from sklearn.inspection import permutation_importance
#
# Get Feature importance data using feature_importances_ attribute
#
feature_importance = gbr.feature_importances_
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
fig = plt.figure(figsize=(8, 8))
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, np.array(X)[sorted_idx])
plt.title('Feature Importance (MDI)')
result = permutation_importance(gbr, X_test_std, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig.tight_layout()
plt.show()

test_score = np.zeros((gbr_params['n_estimators'],), dtype=np.float64)
for i, y_pred in enumerate(gbr.staged_predict(X_test_std)):
    test_score[i] = gbr.loss_(y_test, y_pred)
 
fig = plt.figure(figsize=(8, 8))
plt.subplot(1, 1, 1)
plt.title('Deviance')
plt.plot(np.arange(gbr_params['n_estimators']) + 1, gbr.train_score_, 'b-',
         label='Training Set Deviance')
plt.plot(np.arange(gbr_params['n_estimators']) + 1, test_score, 'r-',
         label='Test Set Deviance')
plt.legend(loc='upper right')
plt.xlabel('Boosting Iterations')
plt.ylabel('Deviance')
fig.tight_layout()
plt.show()

plt.figure(figsize=(12, 8))
# acutal values
plt.plot([i for i in range(len(y_test))],y_test, label="actual values")
# predicted values
plt.plot([i for i in range(len(y_test))],gbr.predict(X_test_std), c='m',label="predicted values")
plt.legend()
plt.show()

print('R score is :', r2_score(y_test, gbr.predict(X_test_std)))